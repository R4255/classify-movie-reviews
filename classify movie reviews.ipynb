{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5ad972-8d77-49eb-802e-07b043228367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rohit\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "218\n",
      "189\n",
      "WARNING:tensorflow:From C:\\Users\\rohit\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          2834688   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2843041 (10.85 MB)\n",
      "Trainable params: 2843041 (10.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\rohit\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\rohit\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\rohit\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "625/625 [==============================] - 35s 51ms/step - loss: 0.4541 - acc: 0.7754 - val_loss: 0.3258 - val_acc: 0.8656\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.2637 - acc: 0.8964 - val_loss: 0.2994 - val_acc: 0.8790\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 32s 51ms/step - loss: 0.1995 - acc: 0.9256 - val_loss: 0.2935 - val_acc: 0.8824\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.1626 - acc: 0.9423 - val_loss: 0.2872 - val_acc: 0.8802\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.1313 - acc: 0.9550 - val_loss: 0.3465 - val_acc: 0.8818\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.1128 - acc: 0.9621 - val_loss: 0.3346 - val_acc: 0.8774\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.0930 - acc: 0.9701 - val_loss: 0.3440 - val_acc: 0.8698\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.0792 - acc: 0.9750 - val_loss: 0.7124 - val_acc: 0.8172\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.0655 - acc: 0.9796 - val_loss: 0.4237 - val_acc: 0.8724\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.0576 - acc: 0.9828 - val_loss: 0.4653 - val_acc: 0.8708\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.5083 - acc: 0.8593\n",
      "[0.5083123445510864, 0.8593199849128723]\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 0s 0us/step\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n",
      "that movie was just amazing so amazing\n",
      "1/1 [==============================] - 1s 588ms/step\n",
      "[0.86460155]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[0.5118739]\n"
     ]
    }
   ],
   "source": [
    "#word embedding are a vectorized representation of words in a given document that places words with similar meanings near each other.\n",
    "#first we need to convert the textual data into the numerical data\n",
    "#it maintains an internal memory or state \n",
    "#rnn does not process the data all at once \n",
    "#in rnn we actually have a loop , we feed one word at a time \n",
    "#simple rnn layer , it takes the output of previous layer , mix with current input and output us \n",
    "#lstm long short term memory , it keep track of all the output at each of the stage\n",
    "#SENTIMENTAL ANALYSIS\n",
    "import keras\n",
    "from keras.datasets import imdb \n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf \n",
    "import os \n",
    "import numpy as np \n",
    "VOCAB_SIZE=88584\n",
    "MAXLEN=250\n",
    "BATCH_SIZE=64\n",
    "(train_data,train_label),(test_data,test_labels)=imdb.load_data(num_words=VOCAB_SIZE)#train_data is simply list of lists of integers\n",
    "print(train_data[0])#just taking a look\n",
    "print(len(train_data[0]))\n",
    "print(len(train_data[1]))\n",
    "#if we look at the length of each of the train_data we will find all of them different\n",
    "#this is an issue we cant pass different length into our neural network\n",
    "'''Therefore, we must make each review the same length. To do this we will follow the procedure below:\n",
    "\n",
    "if the review is greater than 250 words then trim off the extra words\n",
    "if the review is less than 250 words add the necessary amount of 0's to make it equal to 250.\n",
    "Luckily for us keras has a function that can do this for us:'''\n",
    "train_data=sequence.pad_sequences(train_data,MAXLEN)\n",
    "test_data=sequence.pad_sequences(test_data,MAXLEN)\n",
    "#the padding will add to the left side of the text to make it equal to maxlen \n",
    "#print(len(train_data[1]))\n",
    "#print(len(train_data[0]))\n",
    "#print(train_data[0])\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE,32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "''' It first converts each word in the input sequence to a dense vector using an embedding layer. Then, it uses an LSTM layer to learn long-term dependencies in the sequence. Finally, it uses a dense layer with a sigmoid activation function to produce a single output value which could represent the probability of the input belonging to a specific category.'''\n",
    "model.summary()\n",
    "\n",
    "#now its time to compile and train the model \n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "history=model.fit(train_data,train_label,epochs=10,validation_split=0.2)\n",
    "# model's predictions and the true labels (loss)\n",
    "#optimizer: This specifies the algorithm that updates the model's weights to minimize the chosen loss function. 'rmsprop' is a variant of gradient descent optimizer known for its efficiency.\n",
    "#metrics: This defines the metrics used to evaluate the model's performance during training and testing. Here, 'acc' (accuracy) is used to measure the percentage of correctly classified reviews.\n",
    "#model.fit takes four arguements\n",
    "#validation_split: This specifies the proportion of the training data to be used for validation.\n",
    "#now we will evaluate the model on our training data to see how well it performs \n",
    "results=model.evaluate(test_data,test_labels)\n",
    "print(results)\n",
    "\n",
    "# now we want to make the predictions on a movie \n",
    "word_index=imdb.get_word_index()\n",
    "def encode_text(text):\n",
    "    tokens=keras.preprocessing.text.text_to_word_sequence(text)#this is basically tokenization\n",
    "    tokens=[word_index[word] if word in word_index else 0 for word in tokens ]\n",
    "    return sequence.pad_sequences([tokens],MAXLEN)[0]\n",
    "text='that movie was just amazing , so amazing '\n",
    "encoded=encode_text(text)\n",
    "print(encoded)\n",
    "\n",
    "#decode function \n",
    "\n",
    "reverse_word_index={value:key for (key,value) in word_index.items()}\n",
    "def decode_integers(integers):\n",
    "    PAD=0\n",
    "    text=\"\"\n",
    "    for num in integers:\n",
    "        if num!=PAD:\n",
    "            text+=reverse_word_index[num] + \" \"\n",
    "    return text[:-1] #all elements of the list except the last one.\n",
    "print(decode_integers(encoded))\n",
    "\n",
    "#now we make the predictions \n",
    "def predict(text):\n",
    "    encoded_text=encode_text(text)\n",
    "    pred=np.zeros((1,250))\n",
    "    pred[0]=encoded_text\n",
    "    result=model.predict(pred)\n",
    "    print(result[0])\n",
    "positive_review=\"that movie was so awesome ! i loved it and would watch it again because it was amazingly great\"\n",
    "predict(positive_review)\n",
    "negative_review='''that movie sucked . I hated it and wouldn't watch it again.was one of the worst things'''\n",
    "predict(negative_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b26cf-d469-4304-956a-ff7622123782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
